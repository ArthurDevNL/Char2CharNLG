{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5d3e2719e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'e2e-dataset/trainset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'e2e-dataset/devset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('e2e-dataset/trainset.csv', encoding='latin-1')\n",
    "df_test = pd.read_csv('e2e-dataset/devset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mr</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name[The Vaults], eatType[pub], priceRange[mor...</td>\n",
       "      <td>The Vaults pub near Café Adriatic has a 5 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name[The Cambridge Blue], eatType[pub], food[E...</td>\n",
       "      <td>Close to Café Brazil, The Cambridge Blue pub s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name[The Eagle], eatType[coffee shop], food[Ja...</td>\n",
       "      <td>The Eagle is a low rated coffee shop near Burg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name[The Mill], eatType[coffee shop], food[Fre...</td>\n",
       "      <td>Located near The Sorrento is a French Theme ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name[Loch Fyne], food[French], customer rating...</td>\n",
       "      <td>For luxurious French food, the Loch Fyne is lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  mr  \\\n",
       "0  name[The Vaults], eatType[pub], priceRange[mor...   \n",
       "1  name[The Cambridge Blue], eatType[pub], food[E...   \n",
       "2  name[The Eagle], eatType[coffee shop], food[Ja...   \n",
       "3  name[The Mill], eatType[coffee shop], food[Fre...   \n",
       "4  name[Loch Fyne], food[French], customer rating...   \n",
       "\n",
       "                                                 ref  \n",
       "0  The Vaults pub near Café Adriatic has a 5 star...  \n",
       "1  Close to Café Brazil, The Cambridge Blue pub s...  \n",
       "2  The Eagle is a low rated coffee shop near Burg...  \n",
       "3  Located near The Sorrento is a French Theme ea...  \n",
       "4  For luxurious French food, the Loch Fyne is lo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name[The Vaults], eatType[pub], priceRange[more than £30], customer rating[5 out of 5], near[Café Adriatic]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- name[The Eagle]\n",
    "- eatType[coffee shop]\n",
    "- food[French]\n",
    "- priceRange[moderate]\n",
    "- customerRating[3/5]\n",
    "- area[riverside]\n",
    "- kidsFriendly[yes]\n",
    "- near[Burger King]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "types = ['name', 'eatType', 'food', 'priceRange', 'customer rating', 'area', 'kidsFriendly', 'near']\n",
    "for s in df_train.mr:\n",
    "    comps = s.split(',')\n",
    "    for c in comps:\n",
    "        for t in types:\n",
    "            c = c.strip()\n",
    "            if c.startswith(t):\n",
    "                if t not in d:\n",
    "                    d[t] = set()\n",
    "                \n",
    "                val = c[len(t)+1:].replace(']', '')\n",
    "                d[t].add(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': {'city centre', 'riverside'},\n",
       " 'customer rating': {'1 out of 5',\n",
       "  '3 out of 5',\n",
       "  '5 out of 5',\n",
       "  'average',\n",
       "  'high',\n",
       "  'low'},\n",
       " 'eatType': {'coffee shop', 'pub', 'restaurant'},\n",
       " 'food': {'Chinese',\n",
       "  'English',\n",
       "  'Fast food',\n",
       "  'French',\n",
       "  'Indian',\n",
       "  'Italian',\n",
       "  'Japanese'},\n",
       " 'name': {'Alimentum',\n",
       "  'Aromi',\n",
       "  'Bibimbap House',\n",
       "  'Blue Spice',\n",
       "  'Browns Cambridge',\n",
       "  'Clowns',\n",
       "  'Cocum',\n",
       "  'Cotto',\n",
       "  'Fitzbillies',\n",
       "  'Giraffe',\n",
       "  'Green Man',\n",
       "  'Loch Fyne',\n",
       "  'Midsummer House',\n",
       "  'Strada',\n",
       "  'Taste of Cambridge',\n",
       "  'The Cambridge Blue',\n",
       "  'The Cricketers',\n",
       "  'The Dumpling Tree',\n",
       "  'The Eagle',\n",
       "  'The Golden Curry',\n",
       "  'The Golden Palace',\n",
       "  'The Mill',\n",
       "  'The Olive Grove',\n",
       "  'The Phoenix',\n",
       "  'The Plough',\n",
       "  'The Punter',\n",
       "  'The Rice Boat',\n",
       "  'The Twenty Two',\n",
       "  'The Vaults',\n",
       "  'The Waterman',\n",
       "  'The Wrestlers',\n",
       "  'Travellers Rest Beefeater',\n",
       "  'Wildwood',\n",
       "  'Zizzi'},\n",
       " 'near': {'All Bar One',\n",
       "  'Avalon',\n",
       "  'Burger King',\n",
       "  'Café Adriatic',\n",
       "  'Café Brazil',\n",
       "  'Café Rouge',\n",
       "  'Café Sicilia',\n",
       "  'Clare Hall',\n",
       "  'Crowne Plaza Hotel',\n",
       "  'Express by Holiday Inn',\n",
       "  'Rainbow Vegetarian Café',\n",
       "  'Raja Indian Cuisine',\n",
       "  'Ranch',\n",
       "  'The Bakers',\n",
       "  'The Portland Arms',\n",
       "  'The Rice Boat',\n",
       "  'The Six Bells',\n",
       "  'The Sorrento',\n",
       "  'Yippee Noodle Bar'},\n",
       " 'priceRange': {'cheap',\n",
       "  'high',\n",
       "  'less than £20',\n",
       "  'moderate',\n",
       "  'more than £30',\n",
       "  '£20-25'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mapping that converts the mr type to an Id for the feature vector\n",
    "type2id = {'name':0, 'near':1}\n",
    "i = 2\n",
    "for k, v in d.items():\n",
    "    if k not in ['name', 'near']:\n",
    "        for a in v:\n",
    "            type2id[(k,a)] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mr(s):\n",
    "    mr = []\n",
    "    \n",
    "    types = ['name', 'eatType', 'food', 'priceRange', 'customer rating', 'area', 'kidsFriendly', 'near']\n",
    "    comps = s.split(',')\n",
    "    for c in comps:\n",
    "        for t in types:\n",
    "            c = c.strip()\n",
    "            if c.startswith(t):\n",
    "                val = c[len(t)+1:].replace(']', '')\n",
    "                mr.append((t, val))\n",
    "    return mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('area', 'city centre'): 5,\n",
       " ('area', 'riverside'): 6,\n",
       " ('customer rating', '1 out of 5'): 17,\n",
       " ('customer rating', '3 out of 5'): 15,\n",
       " ('customer rating', '5 out of 5'): 16,\n",
       " ('customer rating', 'average'): 18,\n",
       " ('customer rating', 'high'): 13,\n",
       " ('customer rating', 'low'): 14,\n",
       " ('eatType', 'coffee shop'): 4,\n",
       " ('eatType', 'pub'): 2,\n",
       " ('eatType', 'restaurant'): 3,\n",
       " ('food', 'Chinese'): 19,\n",
       " ('food', 'English'): 25,\n",
       " ('food', 'Fast food'): 20,\n",
       " ('food', 'French'): 22,\n",
       " ('food', 'Indian'): 23,\n",
       " ('food', 'Italian'): 21,\n",
       " ('food', 'Japanese'): 24,\n",
       " ('priceRange', 'cheap'): 11,\n",
       " ('priceRange', 'high'): 7,\n",
       " ('priceRange', 'less than £20'): 8,\n",
       " ('priceRange', 'moderate'): 9,\n",
       " ('priceRange', 'more than £30'): 12,\n",
       " ('priceRange', '£20-25'): 10,\n",
       " 'name': 0,\n",
       " 'near': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mrs_train = [process_mr(s) for s in df_train.mr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_vector(mrs):\n",
    "    vec = np.zeros(len(type2id))\n",
    "    for k,v in mrs:\n",
    "        if k in ['name', 'near']:\n",
    "            vec[type2id[k]] = 1\n",
    "        else:\n",
    "            vec[type2id[(k,v)]] = 1\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([to_feature_vector(x) for x in processed_mrs_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42061, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 1.],\n",
       "       [1., 1., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30.',\n",
       "       'Close to Café Brazil, The Cambridge Blue pub serves delicious Tuscan Beef for the cheap price of £10.50. Delicious Pub food.',\n",
       "       'The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than £20 for Japanese food.',\n",
       "       'Located near The Sorrento is a French Theme eatery and coffee shop called The Mill, with a price range at £20-£25 it is in the riverside area.',\n",
       "       'For luxurious French food, the Loch Fyne is located by the river next to The Rice Boat.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head().ref.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since tokens are naively split on whitespaces, there can still be periods and comma's in tokens\n",
    "def process_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(mr, s):\n",
    "    for k,v in mr:\n",
    "        if k == 'name':\n",
    "            s = s.replace(v, 'SLOT_NAME')\n",
    "        elif k == 'near':\n",
    "            s = s.replace(v, 'SLOT_NEAR')\n",
    "    \n",
    "    result = ['<bos>']\n",
    "    \n",
    "    tokens = s.split(' ')\n",
    "    for t in tokens:\n",
    "        if t == '':\n",
    "            continue\n",
    "            \n",
    "        t = t.strip()\n",
    "        if t.startswith('.'):\n",
    "            result.append('.')\n",
    "            t = t[1:].lower()\n",
    "            \n",
    "        if t.startswith(','):\n",
    "            result.append(',')\n",
    "            t = t[1:].lower()\n",
    "        \n",
    "        # If there's a period in the 'token' add it individually\n",
    "        append_period = False\n",
    "        if t.endswith('.'):\n",
    "            append_period = True            \n",
    "            t = t[:len(t)-1].lower()\n",
    "            \n",
    "        # If there's a comma in the 'token' add it individually\n",
    "        append_comma = False\n",
    "        if t.endswith(','):\n",
    "            append_comma = True            \n",
    "            t = t[:len(t)-1].lower()\n",
    "            \n",
    "        append_period = False\n",
    "        if t.endswith('.'):\n",
    "            append_period = True            \n",
    "            t = t[:len(t)-1].lower()\n",
    "        \n",
    "        # Check if there's a comma\n",
    "        if ',' in t:\n",
    "            cms = t.split(',')\n",
    "            for i in range(len(cms)-1):\n",
    "                result.append(cms[i].lower())\n",
    "            t = cms[-1]\n",
    "                    \n",
    "        result.append(t.lower())\n",
    "        \n",
    "        if append_period == True:\n",
    "            result.append('.')\n",
    "        \n",
    "        if append_comma == True:\n",
    "            result.append(',')\n",
    "            \n",
    "    result.append('<eos>')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = [tokenize(processed_mrs_train[i], df_train.iloc[i].ref) for i in range(len(df_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a vocabulary\n",
    "vocab = ['<unk>']\n",
    "for sent in tokenized_sents:\n",
    "    for t in sent:\n",
    "        if t not in vocab:\n",
    "            vocab.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 vocabulary id mapping\n",
    "word2id = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vec(s):\n",
    "    res = []\n",
    "    # t = token in s = sentence\n",
    "    for t in s:\n",
    "        res.append(word2id[t])\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([sent_to_vec(s) for s in tokenized_sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),\n",
       "       array([ 1, 16, 17,  5, 18,  2,  3, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "       20,  3, 29, 15]),\n",
       "       array([ 1,  2, 30,  7, 31, 32, 33, 34,  4,  5, 35, 24, 36, 37, 30, 38, 39,\n",
       "       35, 30, 40, 41, 42, 23, 43, 29, 15]),\n",
       "       ..., array([  1,   2,  30,  72,  95,  61,  49,  74, 124,  15]),\n",
       "       array([  1,   2,  30,  72, 165,  33,  34,   4,   5,  35,  24,  57,  52,\n",
       "        30,  79,  35,  19, 410,  15]),\n",
       "       array([ 1,  2, 30, 72, 65, 61, 53, 24, 80, 81, 15])], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, RepeatVector, Dense, Activation, Input, Flatten, Reshape, Permute, Lambda\n",
    "from keras.layers.merge import multiply, concatenate\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#import seq2seq\n",
    "#from seq2seq.models import AttentionSeq2Seq\n",
    "\n",
    "import data_loader\n",
    "import postprocessing\n",
    "\n",
    "\n",
    "def main():\n",
    "    path_to_data_dir = 'data/'\n",
    "    path_to_embeddings_dir = 'embeddings/'\n",
    "\n",
    "    use_pretrained_embeddings = True        # set to True to use a pre-trained word embedding model\n",
    "    split_mrs = True                        # set to True to split the test MRs before predicting\n",
    "    postprocess = True                      # set to False to skip the utterance post-processing\n",
    "    max_input_seq_len = 30                  # number of words the MRs should be truncated/padded to\n",
    "    max_output_seq_len = 50                 # number of words the utterances should be truncated/padded to\n",
    "    vocab_size = 10000                      # maximum vocabulary size of the utterances\n",
    "    num_variations = 3                      # number of MR permutations to consider for re-ranking\n",
    "    depth_enc = 1                           # number of LSTM layers in the encoder\n",
    "    depth_dec = 1                           # number of LSTM layers in the decoder\n",
    "    hidden_layer_size = 500                 # number of neurons in a single LSTM layer\n",
    "\n",
    "\n",
    "    # ---- WORD EMBEDDING ----\n",
    "    print('\\nLoading embedding model...')\n",
    "    embedding_model = data_loader.load_embedding_model(path_to_data_dir, path_to_embeddings_dir, use_pretrained_embeddings)\n",
    "    weights = embedding_model.syn0\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    print('weights.shape =', weights.shape)\n",
    "    #print(embedding_model.similarity('pizza', 'hamburger'))\n",
    "    #print(embedding_model.similarity('pizza', 'furniture'))\n",
    "    #print(embedding_model.similarity('coffee', 'tea'))\n",
    "\n",
    "\n",
    "    # ---- LOAD DATA ----\n",
    "    print('\\nLoading data...')\n",
    "    #word2idx, idx2word = data_loader.load_vocab(path_to_vocab)\n",
    "    x_train, y_train, x_test, y_test, original_mrs, original_sents, test_groups, y_idx2word = \\\n",
    "            data_loader.load_data(path_to_data_dir, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs)\n",
    "\n",
    "    # x_test, y_test = permute_input(original_mrs, original_sents)\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    print('Utterance vocab size:', len(y_idx2word))\n",
    "    print('x_train.shape =', x_train.shape)\n",
    "    print('y_train.shape =', y_train.shape)\n",
    "    print('x_test.shape =', x_test.shape)\n",
    "    print('y_test.shape =', y_test.shape)\n",
    "\n",
    "\n",
    "    # ---- BUILD THE MODEL ----\n",
    "    print('\\nBuilding language generation model...')\n",
    "    #model = Sequential()\n",
    "\n",
    "    #ret_seq_first_layer = False\n",
    "    #if depth_enc > 1:\n",
    "    #    ret_seq_first_layer = True\n",
    "\n",
    "    ## -- ENCODER --\n",
    "    ##model.add(Embedding(input_dim=weights.shape[0],\n",
    "    ##                    output_dim=weights.shape[1],\n",
    "    ##                    weights=[weights],\n",
    "    ##                    input_length=max_seq_len,       # can be omitted to process sequences of heterogenous length\n",
    "    ##                    trainable=False))\n",
    "    #model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                             dropout=0.2,\n",
    "    #                             recurrent_dropout=0.2,\n",
    "    #                             return_sequences=ret_seq_first_layer),\n",
    "    #                        input_shape=(max_input_seq_len, weights.shape[1])))\n",
    "    #if depth_enc > 2:\n",
    "    #    for d in range(depth_enc - 2):\n",
    "    #        model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                                        dropout=0.2,\n",
    "    #                                        recurrent_dropout=0.2,\n",
    "    #                                     return_sequences=True)))\n",
    "    #if depth_enc > 1:\n",
    "    #    model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                                    dropout=0.2,\n",
    "    #                                    recurrent_dropout=0.2,\n",
    "    #                                 return_sequences=False)))\n",
    "\n",
    "    ## -- DECODER --\n",
    "    #model.add(RepeatVector(max_output_seq_len))\n",
    "    #for d in range(depth_dec):\n",
    "    #    model.add(LSTM(units=weights.shape[1],\n",
    "    #                   dropout=0.2,\n",
    "    #                   recurrent_dropout=0.2,\n",
    "    #                   return_sequences=True))\n",
    "    #model.add(TimeDistributed(Dense(len(y_idx2word),\n",
    "    #                                activation='softmax')))\n",
    "\n",
    "\n",
    "    # ---- ATTENTION MODEL ----\n",
    "\n",
    "    input = Input(shape=(max_input_seq_len, weights.shape[1]))\n",
    "\n",
    "    # -- ENCODER --\n",
    "    encoder = Bidirectional(LSTM(units=hidden_layer_size,\n",
    "                                 dropout=0.2,\n",
    "                                 recurrent_dropout=0.2,\n",
    "                                 return_sequences=True),\n",
    "                            merge_mode='concat')(input)\n",
    "\n",
    "    # -- ATTENTION --\n",
    "    flattened = Flatten()(encoder)\n",
    "\n",
    "    attention = []\n",
    "    for i in range(max_output_seq_len):\n",
    "        weighted = Dense(max_input_seq_len, activation='softmax')(flattened)\n",
    "        unfolded = Permute([2, 1])(RepeatVector(hidden_layer_size * 2)(weighted))\n",
    "        multiplied = multiply([encoder, unfolded])\n",
    "        summed = Lambda(lambda x: K.sum(x, axis=-2))(multiplied)\n",
    "        attention.append(Reshape((1, hidden_layer_size * 2))(summed))\n",
    "\n",
    "    attention_out = concatenate(attention, axis=-2)\n",
    "\n",
    "    # -- DECODER --\n",
    "    decoder = LSTM(units=hidden_layer_size,\n",
    "                   dropout=0.2,\n",
    "                   recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(attention_out)\n",
    "\n",
    "    decoder = Dense(len(y_idx2word),\n",
    "                    activation='softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs=input, outputs=decoder)\n",
    "\n",
    "\n",
    "    # ---- Keras Seq2Seq attention model [https://github.com/farizrahman4u/seq2seq] (not working) ----\n",
    "    #model = AttentionSeq2Seq(input_dim=weights.shape[1],\n",
    "    #                         input_length=max_input_seq_len,\n",
    "    #                         hidden_dim=hidden_layer_size,\n",
    "    #                         output_length=max_output_seq_len,\n",
    "    #                         output_dim=len(y_idx2word),\n",
    "    #                         depth=1)\n",
    "\n",
    "\n",
    "    # ---- COMPILE ----\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # -- Define Checkpoint--\n",
    "    #filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    filepath = 'trained_model.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    \n",
    "    # ---- TRAIN ----\n",
    "    print('\\nTraining...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=20,\n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    \n",
    "    # ---- TEST ----\n",
    "    #print('\\nTesting...')\n",
    "    #score, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "    #print()\n",
    "    #print('-> Test score:', score)\n",
    "    #print('-> Test accuracy:', acc)\n",
    "\n",
    "\n",
    "    # ---- PREDICT ----\n",
    "    print('\\nPredicting...')\n",
    "\n",
    "    # -- SINGLE PREDICTION --\n",
    "    #prediction_distr = model.predict(np.array([x_test[123]]))       # test MR: name[The Rice Boat], food[Japanese], area[city centre]\n",
    "    #prediction = np.argmax(prediction_distr, axis=2)                # note: prediction_distr is a 3D array even for a single input to model.predict()\n",
    "    #utterance = [y_idx2word[idx] for idx in prediction[0] if idx > 0]\n",
    "    #print(' '.join(utterance))\n",
    "\n",
    "    # -- BATCH PREDICTION --\n",
    "    results = []\n",
    "    prediction_distr = model.predict(np.array(x_test))\n",
    "    predictions = np.argmax(prediction_distr, axis=2)\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        utterance = ' '.join([y_idx2word[idx] for idx in prediction if idx > 0])\n",
    "        results.append(utterance)\n",
    "\n",
    "    # print(len(original_mrs))\n",
    "    # print(len(results))\n",
    "    print(\"Predictions have been processed. Now we are depermuting them: \")\n",
    "    # x, y, p = postprocessing.depermute_input(original_mrs, original_sents, results, num_variations)\n",
    "    # correct_preds = postprocessing.correct(x, p)\n",
    "    # print(len(original_mrs))\n",
    "    # print(len(results))\n",
    "    if split_mrs:\n",
    "        results_merged = postprocessing.merge_utterances(results, original_mrs, test_groups, num_variations)\n",
    "    else:\n",
    "        results_merged = []\n",
    "        for i, prediction in enumerate(results):\n",
    "            results_merged.append(postprocessing.relex_utterance(prediction, original_mrs[i]))\n",
    "\n",
    "    #todo add this\n",
    "    # if not split_mrs:\n",
    "    #     utterance = postprocessing.relex_utterance(utterance, original_mrs[i])\n",
    "\n",
    "    np.savetxt('results/results_raw.txt', list(results_merged), fmt='%s')\n",
    "    # print('\\n'.join(results_merged))\n",
    "\n",
    "\n",
    "    # ---- POST-PROCESS ----\n",
    "    if postprocess:\n",
    "        print(\"Predictions have been processed. Now we are depermuting them: \")\n",
    "        x, y, p = postprocessing.depermute_input(original_mrs, original_sents, results_merged, num_variations)\n",
    "        print(\"Depermution is done, files written.\")\n",
    "        print(\"Writing depermute file.\")\n",
    "        cp = postprocessing.combo_print(p, results_merged, num_variations)\n",
    "        correct_preds = postprocessing.correct(x, p)\n",
    "\n",
    "        # for pp in p:\n",
    "        #     print(pp)\n",
    "        np.savetxt('results/results_pooling.txt', list(p), fmt='%s')\n",
    "        np.savetxt('results/results_combo_pool.txt', list(cp), fmt='%s')\n",
    "        np.savetxt('results/results_pooling_corrected.txt', list(correct_preds), fmt='%s')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(int(main() or 0))\n",
    "\n",
    "    # t = \"The Golden Currey serves Fast food food near near\"\n",
    "    # y = \"The Golden Currey is rated a 3 3 of a of 5 5 5 5\"\n",
    "    # x = \"The Golden Currey is near near the city centre\"\n",
    "    # blah = [t, y, x]\n",
    "    # mrs = [0,0,0]\n",
    "    # # t = \"The Golden Currey is a family near near\"\n",
    "    # # t = score_grammar_spelling(t, True)\n",
    "    # tool = language_check.LanguageTool('en-US')\n",
    "    # for g in blah:\n",
    "    #     print(score_grammar_spelling(False, g, tool))\n",
    "    #     print(score_known_errors(g))\n",
    "    # print(correct(mrs, blah))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-9c76f6503399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import embedding\n",
    "\n",
    "# TODO: rewrite into object-oriented\n",
    "\n",
    "def load_embedding_model(path_to_data_dir, path_to_embeddings_dir, use_pretrained_embeddings):\n",
    "    path_to_training = path_to_data_dir + 'trainset_perm_3_slot_mr.csv'\n",
    "    path_to_test = path_to_data_dir + 'devset_3_slot_mr.csv'\n",
    "    path_to_embeddings = path_to_embeddings_dir + 'embeddings.npy'\n",
    "    path_to_vocab = path_to_embeddings_dir + 'vocab.json'\n",
    "    path_to_model = path_to_embeddings_dir + 'embedding_model.bin'\n",
    "    #path_to_pretrained_model = path_to_embeddings_dir + 'GoogleNews-vectors-negative300.bin'\n",
    "    path_to_pretrained_model = path_to_embeddings_dir + 'glove.6B.300d.txt'\n",
    "\n",
    "\n",
    "    if use_pretrained_embeddings:\n",
    "        # load Google's word2vec pre-trained word embedding model\n",
    "        #return KeyedVectors.load_word2vec_format(path_to_pretrained_model, binary=True)\n",
    "\n",
    "        # load Stanford's GloVe pre-trained word embedding model\n",
    "        return KeyedVectors.load_word2vec_format(path_to_pretrained_model, binary=False)\n",
    "    else:\n",
    "        # train custom embedding model, if necessary\n",
    "        if (os.path.isdir(path_to_embeddings_dir) == False or os.path.isfile(path_to_embeddings) == False):\n",
    "            embedding.create_embeddings([path_to_training, path_to_test],\n",
    "                                        path_to_embeddings,\n",
    "                                        path_to_vocab,\n",
    "                                        path_to_model,\n",
    "                                        size=100,\n",
    "                                        min_count=2,\n",
    "                                        window=5,\n",
    "                                        iter=1)\n",
    "\n",
    "        # load our trained word2vec model\n",
    "        return KeyedVectors.load_word2vec_format(path_to_model, binary=False)\n",
    "\n",
    "\n",
    "def load_data(path_to_data_dir, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs):\n",
    "    path_to_training = path_to_data_dir + 'trainset.csv'\n",
    "    #path_to_training = path_to_data_dir + 'trainset_perm_3_slot_mr.csv'\n",
    "    path_to_test = path_to_data_dir + 'devset.csv'\n",
    "    #path_to_test = path_to_data_dir + 'devset_3_slot_mr.csv'\n",
    "    #path_to_data_embed = path_to_data_dir + 'data_embed.pkl'\n",
    "    \n",
    "\n",
    "    # store/load the data in the embedded form\n",
    "    #if os.path.isfile(path_to_data_embed) == False:\n",
    "    #    x_train, y_train, x_test, y_test = preprocess_data(path_to_training, path_to_test, embedding_model, max_seq_len)\n",
    "    #    with open(path_to_data_embed, 'wb') as f:\n",
    "    #        pickle.dump([x_train, y_train, x_test, y_test], f)\n",
    "    #else:\n",
    "    #    with open(path_to_data_embed, 'rb') as f:\n",
    "    #        x_train, y_train, x_test, y_test = pickle.load(f)\n",
    "\n",
    "    #return x_train, y_train, x_test, y_test\n",
    "\n",
    "    return preprocess_data(path_to_training, path_to_test, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs)\n",
    "\n",
    "\n",
    "def preprocess_data(path_to_training_data, path_to_test_data, embedding, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs):\n",
    "    # read the training data from file\n",
    "    data_frame_train = pd.read_csv(path_to_training_data, header=0, encoding='latin1')  # names=['mr', 'ref']\n",
    "    x_train = data_frame_train.mr.tolist()\n",
    "    y_train = data_frame_train.ref.tolist()\n",
    "\n",
    "    # read the test data from file\n",
    "    data_frame_test = pd.read_csv(path_to_test_data, header=0, encoding='latin1')       # names=['mr', 'ref']\n",
    "    x_test = data_frame_test.mr.tolist()\n",
    "    y_test = data_frame_test.ref.tolist()\n",
    "\n",
    "    original_mrs = copy.deepcopy(x_test)\n",
    "    original_sents = copy.deepcopy(y_test)\n",
    "\n",
    "    if use_split_mrs:\n",
    "        # split MRs into shorter ones\n",
    "        x_test, y_test, test_groups = split_mrs(x_test, y_test, num_variations=num_variations)\n",
    "    elif num_variations > 1:\n",
    "        x_test, y_test = permute_input(x_test, y_test, num_permutes=num_variations)\n",
    "        test_groups = []\n",
    "    else:\n",
    "        test_groups = []\n",
    "\n",
    "\n",
    "    # parse the utterances into lists of words\n",
    "    y_train = [preprocess_utterance(y) for y in y_train]\n",
    "    y_test = [preprocess_utterance(y) for y in y_test]\n",
    "\n",
    "    # create utterance vocabulary\n",
    "    distr = FreqDist(np.concatenate(y_train + y_test))\n",
    "    y_vocab = distr.most_common(min(len(distr), vocab_size))        # cap the vocabulary size\n",
    "    y_idx2word = [word[0] for word in y_vocab]\n",
    "    y_idx2word.insert(0, '-PADDING-')\n",
    "    y_idx2word.extend(['&slot_val_name&', '&slot_val_food&', '&slot_val_near&'])\n",
    "    y_idx2word.append('-PERIOD-')\n",
    "    y_idx2word.append('-NA-')\n",
    "    y_word2idx = {word: idx for idx, word in enumerate(y_idx2word)}\n",
    "\n",
    "    delex_data(x_train, y_train, update_data_source=True)\n",
    "    delex_data(x_test, y_test, update_data_source=True)\n",
    "    \n",
    "\n",
    "    padding_vec = np.zeros(embedding.syn0.shape[1])         # embedding vector for \"padding\" words\n",
    "\n",
    "    # produce sequences of embedding vectors from the meaning representations (MRs) in the training set\n",
    "    x_train_seq = []\n",
    "    for mr in x_train:\n",
    "        row_list = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot and convert to embedding\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            row_list.extend([embedding[slot_word] for slot_word in slot.split() if slot_word in embedding.vocab])\n",
    "            # parse the value and convert to embedding\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            row_list.extend([embedding[value_word] for value_word in value.split() if value_word in embedding.vocab])\n",
    "        # add padding\n",
    "        row_list = add_padding(row_list, padding_vec, max_input_seq_len)\n",
    "\n",
    "        x_train_seq.append(row_list)\n",
    "\n",
    "    # produce sequences of one-hot vectors from the reference utterances in the training set\n",
    "    y_train_seq = np.zeros((len(y_train), max_output_seq_len, len(y_word2idx)), dtype=np.int8)\n",
    "    for i, utterance in enumerate(y_train):\n",
    "        for j, word in enumerate(utterance):\n",
    "            # truncate long utterances\n",
    "            if j >= max_output_seq_len:\n",
    "                break\n",
    "\n",
    "            # represent each word with a one-hot vector\n",
    "            if word == '.':\n",
    "                y_train_seq[i][j][y_word2idx['-PERIOD-']] = 1\n",
    "            elif word in y_word2idx:\n",
    "                y_train_seq[i][j][y_word2idx[word]] = 1\n",
    "            else:\n",
    "                y_train_seq[i][j][y_word2idx['-NA-']] = 1\n",
    "\n",
    "        # add padding for short utterances\n",
    "        for j in range(len(utterance), max_output_seq_len):\n",
    "            y_train_seq[i][j][y_word2idx['-PADDING-']] = 1\n",
    "\n",
    "    # produce sequences of embedding vectors from the meaning representations (MRs) in the test set\n",
    "    x_test_seq = []\n",
    "    for mr in x_test:\n",
    "        row_list = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot and convert to embedding\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            row_list.extend([embedding[slot_word] for slot_word in slot.split() if slot_word in embedding.vocab])\n",
    "            # parse the value and convert to embedding\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            row_list.extend([embedding[value_word] for value_word in value.split() if value_word in embedding.vocab])\n",
    "        # add padding\n",
    "        row_list = add_padding(row_list, padding_vec, max_input_seq_len)\n",
    "\n",
    "        x_test_seq.append(row_list)\n",
    "\n",
    "    # produce sequences of one-hot vectors from the reference utterances in the test set\n",
    "    y_test_seq = np.zeros((len(y_test), max_output_seq_len, len(y_word2idx)), dtype=np.int8)\n",
    "    for i, utterance in enumerate(y_test):\n",
    "        for j, word in enumerate(utterance):\n",
    "            # truncate long utterances\n",
    "            if j >= max_output_seq_len:\n",
    "                break\n",
    "\n",
    "            # represent each word with a one-hot vector\n",
    "            if word in y_word2idx:\n",
    "                y_test_seq[i][j][y_word2idx[word]] = 1\n",
    "            else:\n",
    "                y_test_seq[i][j][y_word2idx['-NA-']] = 1\n",
    "\n",
    "        # add padding for short utterances\n",
    "        for j in range(len(utterance), max_output_seq_len):\n",
    "            y_test_seq[i][j][y_word2idx['-PADDING-']] = 1\n",
    "\n",
    "    return (np.array(x_train_seq), np.array(y_train_seq), np.array(x_test_seq), np.array(y_test_seq), original_mrs, original_sents, test_groups, y_idx2word)\n",
    "\n",
    "\n",
    "def permute_input(mrs, sents, num_permutes):\n",
    "    new_mr = []\n",
    "    new_sent = []\n",
    "    for x, mr in enumerate(mrs):\n",
    "        sentence = sents[x]\n",
    "        temp = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            temp.append(slot + '[' + value + ']')\n",
    "        for t in range(0, num_permutes):\n",
    "            temptemp = copy.deepcopy(temp)\n",
    "            random.shuffle(temptemp)\n",
    "            curr_mr = ', '.join(temptemp)\n",
    "            new_mr.append(curr_mr)\n",
    "            new_sent.append(sentence)\n",
    "    return new_mr, new_sent\n",
    "\n",
    "\n",
    "def split_mrs(mrs, utterances, num_variations):\n",
    "    new_mrs = []\n",
    "    new_utterances = []\n",
    "    groups = []\n",
    "    group_id = 0\n",
    "\n",
    "    for idx, mr in enumerate(mrs):\n",
    "        utterance = utterances[idx]\n",
    "        # do not split short MRs\n",
    "        if len(mr) < 4:\n",
    "            new_mrs.append(mr)\n",
    "            new_utterances.append(utterance)\n",
    "            continue\n",
    "\n",
    "        slot_value_list = []\n",
    "        name_slot = ()\n",
    "\n",
    "        # parse the slot-value pairs\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "\n",
    "            if slot == 'name':\n",
    "                name_slot = (slot, value)\n",
    "            else:\n",
    "                slot_value_list.append((slot, value))\n",
    "\n",
    "        for i in range(num_variations):\n",
    "            slot_value_list_copy = slot_value_list[:]\n",
    "            random.shuffle(slot_value_list_copy)\n",
    "\n",
    "            # distribute the slot-value pairs as multiple shorter MRs\n",
    "            while len(slot_value_list_copy) > 0:\n",
    "                # include the name slot by default in each subset\n",
    "                mr_subset = [name_slot]\n",
    "                # add up to two other slots to the subset\n",
    "                for i in range(min(2, len(slot_value_list_copy))):\n",
    "                    mr_subset.append(slot_value_list_copy.pop())\n",
    "            \n",
    "                new_mr = [s + '[' + v + ']' for s, v in mr_subset]\n",
    "                new_mrs.append(', '.join(new_mr))\n",
    "                new_utterances.append(utterance)\n",
    "                groups.append(group_id)\n",
    "            \n",
    "            group_id += 1\n",
    "\n",
    "    return new_mrs, new_utterances, groups\n",
    "\n",
    "\n",
    "def preprocess_utterance(utterance, keep_periods=False):\n",
    "    if keep_periods:\n",
    "        chars_to_filter = '!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    \n",
    "        # add spaces before periods so they can be parsed as individual words\n",
    "        utterance = utterance.replace('. ', ' . ')\n",
    "        if utterance[-1] == '.':\n",
    "            utterance = utterance[:-1] + ' ' + utterance[-1]\n",
    "\n",
    "        return text_to_word_sequence(utterance, filters=chars_to_filter)\n",
    "    else:\n",
    "        chars_to_filter = '.!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "        return text_to_word_sequence(utterance, filters=chars_to_filter)\n",
    "\n",
    "\n",
    "def delex_data(mrs, sentences, update_data_source=False, specific_slots=None, split=True):\n",
    "    if specific_slots is not None:\n",
    "        delex_slots = specific_slots\n",
    "    else:\n",
    "        delex_slots = ['name', 'food', 'near']\n",
    "\n",
    "    for x, mr in enumerate(mrs):\n",
    "        if split:\n",
    "            sentence = ' '.join(sentences[x])\n",
    "        else:\n",
    "            sentence = sentences[x].lower()\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            if slot in delex_slots:\n",
    "                value = slot_value[sep_idx + 1:-1].strip()\n",
    "                sentence = sentence.replace(value.lower(), '&slot_val_{0}&'.format(slot))\n",
    "                mr = mr.replace(value, '&slot_val_{0}&'.format(slot))\n",
    "                # if not split:\n",
    "                #     print(\"delex:\")\n",
    "                #     print('&slot_val_{0}&'.format(slot))\n",
    "                #     print(value.lower())\n",
    "                #     print(sentence)\n",
    "        if update_data_source:\n",
    "            if split:\n",
    "                sentences[x] = sentence.split()\n",
    "            else:\n",
    "                sentences[x] = sentence\n",
    "            mrs[x] = mr\n",
    "        if not split:\n",
    "            return sentence\n",
    "        # new_sent = relex_sentences(mr, sentence)\n",
    "\n",
    "\n",
    "def add_padding(seq, padding_vec, max_seq_len):\n",
    "    diff = max_seq_len - len(seq)\n",
    "    if diff > 0:\n",
    "        # pad short sequences\n",
    "        return seq + [padding_vec for i in range(diff)]\n",
    "    else:\n",
    "        # truncate long sequences\n",
    "        return seq[:max_seq_len]\n",
    "\n",
    "\n",
    "def load_vocab(path_to_vocab):\n",
    "    with open(path_to_vocab, 'r') as f_vocab:\n",
    "        data = json.loads(f_vocab.read())\n",
    "\n",
    "    word2idx = data\n",
    "    idx2word = {v: k for k, v in data.items()}\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
